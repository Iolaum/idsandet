% http://tex.stackexchange.com/questions/192817/two-sided-amsbook-with-custom-title-page
% for reference
% \documentclass[twoside, reqno, openright ,12pt]{amsbook}
 

\documentclass[reqno,openany,12pt]{amsbook}
%  openany option dumps the blank pages between chapters when the next
%  chapter starts on odd page number; 
%  following command had similar effect
%  \let\cleardoublepage\clearpage
%  NB need an abstract or get a blank page between title and contents

%  reqno option:
%  right-aligned equation numbers
%  (altenratively leqno)

\usepackage[english]{babel}
% package for language and hyphenation support (?)

\usepackage{amsmath}

%\renewcommand{\baselinestretch}{1.35}
% changed from:
% http://tex.stackexchange.com/a/79155
%\usepackage{setspace}
%\setstretch{1.5}

\usepackage{url}
% package for url links

\usepackage{graphics}
% introduced to resize big table of attack types
% http://tex.stackexchange.com/questions/10863/is-there-a-way-to-slightly-shrink-a-table-including-font-size-to-fit-within-th

\usepackage{pgfgantt}
% package to create gantt charts

% Insert multiple figures in Latex
% http://tex.stackexchange.com/a/119907/111537
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{graphicx}


% Displaying Linux commands in LaTeX [duplicate]
% http://tex.stackexchange.com/a/84188/111537
\usepackage{listings}


%  ************  begin my definitions  *******************

\renewcommand{\thesection}{\thechapter.\arabic{section}}
% http://tex.stackexchange.com/questions/20837/section-numbering-with-chapter-in-amsbook
% show chapter numbers in sections!


%  ************  end my definitions  *******************


% title info
% fix me later

\title{Intrusion Detection using Outliers in a modern Cyber Security Dataset}
\author{Nikolaos Perrakis\\
{\small
MSc Project\\[-1 ex]
University of Southampton\\[-1 ex]
Faculty of Physical Sciences and Engineering\\[-1 ex]
Electronics and Computer Science\\[-1 ex]
}
}


\begin{document}


\maketitle

\frontmatter


\chapter*{Abstract}
\setcounter{page}{1}
This is the summary of the project describing the details of the analysis on the ADFA-LD 12 dataset.



\chapter*{Acknowledgements}

Thank you!!


\tableofcontents
\listoffigures
\listoftables


\mainmatter

\chapter{Introduction}

In the industry we are currently experiencing what many people call the fourth industrial revolution. The main characteristics of this disruptive process are:
\begin{itemize}
\item The ubiquitous presence of connected devices, collectively called Internet of Everything.
\item The ability of cyber system to interact with and affect the physical world creating the so called cyber-physical systems (CPS).
\item The growth of distributed computing and the capabilities that it allows.
\item The evolution of Machine Learning enabling cyber-physical systems with increased autonomy.\footnote{A good example of an autonomous cyber-physical system are Tesla's self-driving cars.}
\end{itemize}
One common denominator of the aspects of the fourth industrial revolution is increased connectivity of computing devices. This brings the negative side effect that more facets of our life and society are exposed online making cyber-security even more important. During the last couple of years we have seen many examples of this danger.

Further expanding on the example of cars we mention that on the summer of 2015 two american security researchers demonstrated that a contemporary model Jeep Cherokee could be remotely accessed maliciously over mobile telephony network.\cite{intro-car1} As demonstrated in the afforementioned article\cite{intro-car1} this has resulted in the academia, the industry and regulators considering policy decisions for regulation and standards adoption in order for car safety to keep up with technological innovation. Another significant event is the attack on a regional electricity distribution company, Ukrainian Kyivoblenergo, on 23 December 2015. The attack compromised the company's computer systems and their supervisory control and data acquisition systems (SCADA). It resulted in power outages for around 225,000 customers for a period of hours. The diligence of the company employees and the quick transition on 
manual mode allowed the company to restore its service with little delay. The attack is thought to have been carried out by a state actor and could have inflicted more damage had the perpetrators more time before making their pressence known. Again this attack has attracted a lot of attention among security researchers and the industry in order to formulate appropriate policies to protect core infrastructure\cite{intro-cps1}.

Now that we have seen examples of the increasing importance of cyber - security let's look deeper at what it actually is. Cyber security generally consists of two parts. Intrusion Prevention Systems (IPS) and Intrusion Detection Systems (IDS). Put it simply an IPS prevents the attacker from getting in and an IDS detects him once he is in. With the emergence of cloud computing the boundaries between the two are getting blurred\cite{intro-ids1}. Nowadays modern IDS have a wide range of features including the detection of an occuring attack, automated responses and detection of malicious behavior within the system\cite{intro-ids1}.


\chapter{Foundations of the project}

In this project we are drawing knowledge from two related but distinct fields.
\footnote{With info from:\\
Project Preparation: Outlier Detection in Cybersecurity Application, 2016, submitted by Nikolaos Perrakis.}
The first field is outlier and anomaly detection techniques from Machine Learning. The second field is Intrusion Detection techniques within deployed cyber-security solutions. On the next sections we will describe the foundations of this project. They have been studied to a big extend during the Project preparation course.

\section{Outlier and Anomaly Detection}

The concept of bad data has been quite old in the field of statistics. One of the first systematic approaches to dealing with them, dating back to the $19^{th}$ century has been Chauvenet's criterion on  whether one data point of an experimental data set was spurious or not. Moving on to modern machine learning techniques on outlier and anomaly detection we see that there are many approaches. They are based on different statistical methods but also on different characteristic on the domain from which the dataset comes. Below are most relevant methods with regards to our work.

The first method has been put forward by Roberts et al.\cite{out1}. They use a Gaussian Mixture model to model the normal behaviour of the system their dataset describes. This creates a set of normal system states. They minimise the set of heuristically chosen parameters used in such techniques by using an evolving threshold on how much a data point can differ from learnt normal state before it gets classified as belonging to another state. When we are training out model this results in a new learnt normal state. Afterwards the threshold is used to identify an anomaly in the test data. They then test their method in a set of electroencephalograms (EEG) obtained from patient data. They prove the robustness of their method by successfully identifying epileptic seizures when they occur.

A popular technique in Machine Learning is Support Vector Machines. Hence it is no surprise that there are several outlier and anomaly detection methods developed with this technique. Clifton et al.\cite{out2} study luminosity measurements of a Typhoon G30 combustor engine and create an SVM model for anomaly detection. They use wavelet analysis on the multi channel combustion data to create their feature space and subsequently perform supervised learning to train their SVM to recognise anomalous behaviour. Clifton et al. also compare the results of the SVM approach to the GMM approach on the same dataset. They found that SVM based anomaly detection performed better. Hayton et al.\cite{out3} also studied SVM based anomaly detection in Jet engine vibration spectra. Their approach also adds the feature of combining a second dataset in order to more accurately train their model. Moreover during the discussion of their results Hayton et al. give a very insightful discussion on whether it is preferable to address novelty detection as a 2-class classification problem or not. Two key points that they make are that the novel data points may be artificial and that their nature may be non-stationary. This is also the case with intrusion detection systems which is why deployed Intrusion Detection Systems use anomaly detection approaches. 

We can also use unsupervised learning approaches when performing anomaly detection. This is pioneered in the work of Schölkopf et al.\cite{out4}. They propose an algorithm that estimates the region where the probability density of some given data lives. New data can be compared to this density for anomaly detection purposes. They describe their method in detail and describe it's conceptual limitations. Then they demonstrate it's characteristics in an artificial dataset and it's effectiveness in a real dataset.

\section{Intrusion Detection Systems}

Bhuyan et al.\cite{ids1} give the following description: ``Intrusion is a set of actions aimed to compromise the security of computer and network components in terms of confidentiality, integrity and availability". Intrusion Detection Systems are our answer to that threat. The basic assumption we make is that our system will behave differently during an intrusion than during normal operation. And this is why it is appropriate to use novelty detection in IDS applications.

There are many attack actions against computer system that can be classified as intrusions with the above definition. Each of them has different specific characteristics. Thus it is helpful for us to divide attacks into certain classes. As we did during Project Preparation\footnote{Project Preparation: Outlier Detection in Cybersecurity Application, 2016, submitted by Nikolaos Perrakis.} we will use the classification used in Bhuyan et al.\cite{ids1} and Ahmed et al.\cite{ids2}. We classify intrusions as Malware attacks, Denial of Service attacks, Network attacks, Physical attacks, Password Attacks, Information Gathering Attacks, Remote to User attacks and User to Root attacks.
Another important property of intrusions is that they have varying anomaly characteristics. We therefore classify them in an additional way. There are \emph{point anomalies} when a data point is considered anomalous when it is distant in comparison to the points that model the normal behaviour of the system. This type of intrusion fits closely the Machine Learning problem of anomaly detection and it will be the main focus of our work. There are also \emph{contextual anomalies} when a data point may be considered anomalous according to the context it is associated as well as it's place in the feature space. The context may be additional features engineered in our feature space or an evaluation of circumstances associated with the data point from the IDS (or it's operator). Lastly there are \emph{collective anomalies} for which a collection of data is considered anomalous but each data point, taken individually, is not. In Table \ref{tab1}\footnote{Table taken from: Project Preparation: Outlier Detection in Cybersecurity Application, 2016, Nikolaos Perrakis} we describe the eight classes of attacks, give examples of them and classify the examples with their respective type of anomalies.

% Full page width table
\begin{table*}
\centering
\caption{Types of attacks and associated anomalies.}
%
\resizebox{\textwidth}{!}{%
%
\begin{tabular}{|c|l|l|} \hline
Attack Type & \phantom{1111111111111111111111111111} Description & Examples (Anomaly Type)\\ \hline
Malware  & 
\begin{tabular}{@{}c@{}}
Virus, Worm, Troyan: A program that may replicate and transfer on \\ its own and  performs harmful operations on the infected computer.  
\end{tabular}
& Stuxnet Worm (point) \\ \hline
Denial of Service & Attacks that make Network resources inaccessible. & Smurf (collective) \\ \hline
Network & Compromising the security of a Network by exploiting Network protocols. & Man-in-the-Middle (point)\\ \hline
Physical  & Compromising a system or a network through physical access. & Evil Maid (point) \\ \hline
Password & Trying to find a user's password, usually through multiple login attempts. & Dictionary attack (collective) \\ \hline
Information Gathering Attacks & Gathering information trying to find vulnerabilities in a network.  & Port scan (collective) \\ \hline
Remote to User & Trying to get remote access as a user to a system.  & phf (point) \\ \hline
User to Root & Upgrading a user's privilege to superuser. & Rootkit (point)\\ \hline
\end{tabular}
} % end resize
\label{tab1}
\end{table*}
% end the environment with {table*}, not {table}!

An operational enterprise IDS solution consists of main parts. Two main parts are a Network Intrusion Detection System (NIDS) which analyses traffic over a company's network to detect intrusion and a Host-based Intrusion Detection System (HIDS) which is installed on the company's computers (hosts) and monitors them in order to detect intrusion.

There are many techniques used to identify anomalies on an IDS. Bhuyan et al.\cite{ids1} do a good work on categorizing them and we will follow their classification scheme in presenting them. It should be noted however that these methods are not completely distinct from each other and a particular implementation may include aspects from more than one.
The first class is statistical methods, which also includes Bayesian Networks. A model is trained to ``learn" the normal operation of the system. A threshold of statistical distance from normal operation is determined and data points exceeding that threshold are classified as anomalous.
The second class is clustering methods. A distance or similarity method has to be defined on the feature space. It is then used to cluster the data with various algorithms such as k-means clustering. We then measure the distance of new data points with our learnt clusters and use it to classify them as anomalous or not.
The third class is classification methods. They include both supervised and unsupervised algorithms. One good example, that we will use later, are support vector machines (SVM). They can be trained either on pre-labelled data or at non-labelled data depending on the SVM implementation.
The fourth class is knowledge based methods. They take advantage of what has been learnt from previous attacks so that they are able to identify them when they re-appear. Some examples are, ontology, signature based and logic based approaches.
The fifth class are soft computing methods. The key point behind them is that if finding the exact solution is not feasible then we can look for approximate solutions. According to Bhuyan et al.\cite{ids1} artificial neural networks are classified as soft computing methods.
The last class is called combination learner methods and basically consists of techniques that combine more than one of the previous classes in their implementation.


The variety of methods for Intrusion detection can be attributed to their diverse strengths and weaknesses. Additionally the intrusions each IT infrastructure faces are different which means that different IT networks are best suited to different Intrusion Detection methods. For example clustering and Nearest neighbour algorithms have poor performance on high dimensional data because in those cases distance methods cannot accurately differentiate between anomalous and normal data points. One can try to overcome this with feature reduction techniques, such as spectral techniques or principal component analysis, but he has to be careful to maintain the separation between anomalous and normal data points. In general classification techniques suffer because of the need to pre-label data points. Creating those labels is hard and often-times artificial. This results in them trained model becoming outdated quite fast because of the fast paced evolution of the IT field. Using partially labelled data for semi-supervised clustering techniques can be more efficient than classification methods provided we have a feature space with a good distance measure. If not statistical techniques are a better option. As we mentioned another point to consider is how easy it is to update our application. Statistical, clustering and classification methods are all hard to train. But it can be done off-line and their testing phase is fast. Last but not least there are times when the assumption that intrusion events are rare compared to normal events is not true. In that case an intrusion detection method may end up with a high false positive rate. This is a big problem because it interrupts the normal operation of the user.

As we see Intrusion Detection is a very complex problem.  Each IT system has different characteristics and they are better addressed by different techniques. This means that addressing Intrusion Detection to it's entirety goes well far beyond the scope of this project. Therefore we will limit ourselves to a particular case.


\section{Datasets}

A critical part in creating an IDS application is the data you use to train it on. In the enterprise world you have access to the company's data. In the academia however good datasets for IDS are hard to find. One of the first attempta to solve that problem was initiated by DARPA and it resulted at the DARPA 1999 IDS Dataset\footnote{DARPA IDS 1998 - 1999 Datasets: \url{https://www.ll.mit.edu/ideval/data/} - Accessed at 9 Aug 2016}. Even though these datasets have been a standard in the academia for more than a decade they have been heavily criticized as well. Apart from it being outdated several researchers have found artefacts of the simulation used to create the datasets within the data. Moreover the software used to create the dataset is no longer relevant and even at it's time it didn't have a significant market share. Lastly the documentation of the dataset is questioned with some researchers suggesting that the number of attacks present in the dataset is inaccurate.

Over time other datasets started to emerge but none has managed to be come a standard. One reason for this is that the diverse needs of an IDS system means that different datasets address different aspects. This is the case for the dataset we will be using as well. Our dataset addresses host-based intrusion detection (HIDS). It is called ADFA LD-12 and it was presented by Creech et al.\cite{dat2}. We will describe it in detail in the next chapter.

\section{Project Planning}

An important part of any project is time management. To assist us in better management we followed the Gantt Chart shown in figure \ref{gantt}. It is separated in eight parts. Initially we explore the dataset, set up our system and decide the architecture of our workflow.
\begin{figure}[h]
\centering
% Gantt chart initial code:
% http://tex.stackexchange.com/a/133056/111537
\resizebox{\textwidth}{!}{
\begin{ganttchart}
[
x unit=0.4cm,
%y unit title=0.7cm,
%y unit chart=0.8cm,
y unit title=0.4cm,
y unit chart=0.5cm,
vgrid,hgrid, 
title label anchor/.style={below=-1.6ex},
title left shift=.05,
title right shift=-.05,
title height=1,
bar/.style={fill=gray!50},
incomplete/.style={fill=white},
progress label text={},
bar height=0.7,
group right shift=0,
group top shift=.6,
group height=.3,
group peaks height =.2
%progress label text=  {\quad\pgfmathprintnumber[precision=0,verbatim]{#1}\%}
]
{1}{26}
%labels
\gantttitle{MSc Project - Weekly Schedule}{26} \\
\gantttitle{1}{2} 
\gantttitle{2}{2} 
\gantttitle{3}{2} 
\gantttitle{4}{2} 
\gantttitle{5}{2} 
\gantttitle{6}{2} 
\gantttitle{7}{2} 
\gantttitle{8}{2}  
\gantttitle{9}{2}   
\gantttitle{10}{2}
\gantttitle{11}{2}
\gantttitle{12}{2}
\gantttitle{13}{2} \\
%tasks
\ganttbar{Dataset exploration}{1}{4} \\
\ganttbar{Results replication}{3}{6} \\
\ganttbar{Frequency algorithms}{6}{11} \\
\ganttbar{2-sequence algorithms}{11}{16} \\
\ganttbar{Initial report}{15}{20} \\
\ganttbar{Feedback}{21}{23} \\
\ganttbar{Final report}{23}{24} \\
\ganttbar{Demonstration}{25}{26}
%relations
\ganttlink{elem0}{elem1} 
\ganttlink{elem1}{elem3} 
\ganttlink{elem1}{elem2} 
\ganttlink{elem3}{elem4} 
\ganttlink{elem2}{elem4} 
\ganttlink{elem4}{elem5} 
\ganttlink{elem5}{elem6} 
\ganttlink{elem6}{elem7} 
\end{ganttchart}
}
\label{gantt}
\caption{MSc Project Gantt Chart}
\end{figure}
%
%Gantt chart weeks:
%
%Week  1: 06.6 - 10.6
%Week  2: 13.6 - 17.6
%Week  3: 20.6 - 24.6
%Week  4: 27.6 - 01.7
%Week  5: 04.7 - 08.7
%Week  6: 11.7 - 15.7
%Week  7: 18.7 - 22.7
%Week  8: 25.7 - 29.7
%Week  9: 01.8 - 05.8
%Week 10: 08.8 - 12.8
%Week 11: 15.8 - 19.8
%Week 12: 22.8 - 26.8
%Week 13: 29.8 - 02.9
After that we worked on replicating results of previous papers\cite{adf1}, \cite{adf2}. This work overlapped with the work on frequency based algorithms and 2-sequence based algorithms. Once we replicated the work of previous papers we worked on new ways to analyse the dataset. One way of doing so was to get out of the boundaries established by the computer science community in handling the dataset and using it in ways more conventional to the machine learning community. After that we proceeded in writing the initial report and deliver a draft to the first supervisor. Subsequently we used his feedback to improve our analysis and our report. Lastly we prepared a presentation in order to present our work to the second supervisor.


\chapter{Exploratory Data Analysis}

\section{Dataset Documentation}

The dataset we will use for this project is a modern dataset. It has been presented in 2013 IEEE Wireless Communications and Networking Conference\cite{dat2}. It uses modern software that simulates real user cases in the IT landscape. More specifically Ubuntu 11.04 is used as the server operating system. In order to enable intrusion from the internet Apache v$2.2.17$ and PHP v$5.3.5$ were also installed.  Apart from them file transfer protocol (ftp), secure shell (ssh) and MySQL v$14.14$ were enabled. Their default configuration settings were used. Lastly a web based collaborative tool, Tiki Wiki v8.1 was installed and enabled. This version has a documented vulnurenability\footnote{Packet Storm: All things security, \url{https://packetstormsecurity.com/files/108036/INFOSERVE-ADV2011-07.txt}, Accessed 9 August 2016.} that allows web exploitation. Those settings are representative of a local server offering basic web services on the internet. Tiki Wiki's known vulnerability represents the ever present danger of previously unknown vulnerabilities on up to date software running on production infrastructure.

A program called \emph{auditd} was used to monitor kernel system call traces. System call traces are API's provided by the kernel of the operating system for userspace applications to access. Ubutnu $11.04$ uses the linux kernel version $2.6.38$ which has $325$ system calls available\cite{adf1}. The researchers who created the dataset used 6 different types of attacks to compromise their server. They are presented in table \ref{tab2} which was taken from \cite{dat2}.
\begin{table}
\begin{tabular}{|c|c|}
\hline
Payload/Effect & Vector \\ \hline 
Password brute force  & ftp by hydra \\ \hline
Password brute force & ssh by hydra \\ \hline
Add new superuser & Client side poison executable\\ \hline
Java based meterpreter & Tiki Wiki Vulnerability exploit\\ \hline
Linux meterpreter payload & Client sidepoison executable\\ \hline
C100 Webshell & Php remote file inclusion vulnerability \\ \hline
\end{tabular}
\vspace{5pt}
\caption{Attacks used in ADFA-LD 12 dataset.}
\label{tab2}
\end{table}
The dataset is separated in three sets, the training set, the attack set and the validation set. The training and the validation set contain sequences of system calls from the normal operation of the system and the attack set contains the intrusions performed by the creators of the dataset. Each individual attack method is carried out ten times. Each data point of the dataset consists of a series of system calls. Table \ref{tab3} shows the number of data points per subset of ADFA-LD 12.
\begin{table}
\begin{tabular}{|c|c|}
\hline
Subset &  Data points \\ \hline 
Training  & 833 \\ \hline
Validation & 4372 \\ \hline
Attack &  719\\ \hline
\end{tabular}
\vspace{5pt}
\caption{Subsets of ADFA-LD 12 dataset.}
\label{tab3}
\end{table}

The traditional approach on the cyber security field is to use the training set to train a model, use the attack set to find it's accuracy and use the validation set to find it's false positive rate. While we will use that schema we will not limit ourselves to it.

\section{Methods and Technologies used}

Before moving further we would like to mention the technologies we used for this project. Our main computing platform is a x86\_{}64 machine running Ubuntu 16.04.1 LTS\footnote{Ubuntu is an open-source operating system using the Linux kernel, based on Debian and developed by Canonical Inc., \url{http://releases.ubuntu.com/16.04/} - Accessed at August 10, 2016.} as it's operating system. For our analysis we are using the python\cite{pyt} programming language, version 2.7.12. Furthermore to implement our machine learning algorithms we relied on the numpy\cite{numpy}, version 1.11.0, and scikit-learn\cite{skl}, version 0.17.1\footnote{An important note here is that pickled scikit-learn objects may be incompatible with other scikit-learn versions.}, open source libraries. For visualization we relied on the matplotlib\cite{matpl} library version 1.5.1. The code used for the generation of all the results and graphics of this project is publicly available as a github repository.\footnote{The repository is available there: \url{https://github.com/Iolaum/idsandet}}. In order to write our report and presentation we used pdfLaTeX and Texmaker. Last but not least we used git, version 2.7.4, as a version control tool.


In order to streamline our work flow we structured our working repository in the following way. We use / to denote the root of our repository. We put out all of our data files in the /data/ folder and configure .gitignore to files in it. For our analysis we use the /scriprs/ folder where we place all our python scripts. The plots we create are put on the /pictures/ folder which is also ignored. Lastly we wrote the report and the presentation on the /report/ folder configuring .gitignore in a way as to only monitor the two .tex files only (and not the auxiliary files used by the pdfLaTeX compiler). This allows an efficient use of git with github allowing us to track important changes more efficiently.

Since we are not building an application that is to be deployed in a production setting we use a more basic approach for our coding requirements. We create a lot of individual scripts that perform one small task. This gradual progress allows us to monitor keep a very good understanding of our progress because every step we make is assessed before we move to a new one. Another benefit of our modular approach is our ability to test our code. Writing unit tests for our work is inappropriate. Therefore we test manually\footnote{In most cases manual testing means that we run a script in a small subset of our dataset and make sure the result is correct. Then we proceed to finalize the script in order to process the whole dataset. Whenever possible we run numerical tests on the final results to verify them.}. The fact that our scripts are small means that we have to test for very few possibilities and the fact that we run our scripts one after another means those possibilities are added instead of multiplied keeping the manual testing feasible and not counter productive. Moreover this segmentation makes it easier to alter our implementation of a particular algorithm when we want to experiment further with our dataset.

As an additional remark we want to mention that  since python is able to run in a variety of operating systems our results can be replicated on a variety of operating systems. All one has to do is install python and our required libraries on his systems. Then he can download the dataset and place them in the /data/ folder according to the specifications set by the script\footnote{The scripts ``a2\_{}train.py", ``a3\_{}attack.py" and ``a4\_{}validation.py" are the ones who are used to initiate our analysis.} in their initial comments section. After the initial scripts have run the user can be sure he has set up his system properly to reproduce all of our results by running the remaining scripts.

\section{Preprocessing and Visualization}

We start by downloading ADFA-LD 12 Dataset\footnote{Download url: \url{https://www.unsw.adfa.edu.au/australian-centre-for-cyber-security/cybersecurity/ADFA-IDS-Datasets/} - Accessed on 10 August 2016}. After we unzip the files we see that we create 3 folders for each subset of the dataset. The training and validation set have a long list of text files in their respective folders. Each text file represents a data point and contains a series of integers separated by white spaces that correspond to kernel system calls. The attack set is structured in folders according to the type of attack and within them are the text files describing the attack data points.

After we have downloaded the dataset our first order of business is to transform it in a way that it will be easier for us to handle. Given the size of our dataset we decided to handle it locally. For that reason we used pickle and numpy libraries to save python objects that represented the dataset on our hard disk. Our first preprocessing step is to create a python dictionary for each of the subsets containing the information about the sequence of system calls for each data point. Through this process we create the following three pickle files:
\begin{center}
1\_{}training.p  ,
1\_{}attack.p ,
1\_{}validation.p
\end{center}
After that we delete the unzipped dataset files - that were too cumbersome to use - and use those 3 files to load our dataset for further computations.

Our next step is to do basic exploration in our dataset. The kernel of the host operating system provides 325 system calls but we are not sure if all are represented and how that representation is distributed. As a first step we run through our dataset once and create a set\footnote{A set, in python, is an object with the useful property that it does not allow duplicate items.} where we add all the system calls we encounter. Thus we end up with the following list of 175 system calls present in our dataset:

\noindent
[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 173, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 322, 324, 328, 331, 332, 340]

\noindent
A curious thing to mention here is the presence of integers higher than 325. One would think that since we only have 325 system calls only integers up to 325 would be used.\footnote{Nothing in the documentation of the dataset indicated this would (or would not) happen.} This is a very good example that no assumptions should be made when addressing a dataset and that we should always clean and validate the form of the data we expect to have for the next step of our pipeline.


\begin{figure}
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-1.eps}} 
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-2.eps}} \    
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-3.eps}}
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-4.eps}} 
\caption{System Calls distribution.}
\label{syscalls1}
\end{figure}
\begin{figure}
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-5.eps}} 
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-6.eps}} \    
\subfloat{\includegraphics[width = 0.5\textwidth]{a17-syscalls-7.eps}}
\caption{System Calls distribution.}
\label{syscalls2}
\end{figure}

Moving forward we count the presences of each system call in our dataset. The result of this computation is presented on figures \ref{syscalls1} and \ref{syscalls2}. As we see we had to use logarithmic scale for our x axis because of how unevenly the distribution of our system calls is. Moreover we observe that the norm is that system calls are present in all three subsets hinting that there is little room for associating particular system calls with malicious behaviour.

\begin{figure}
\includegraphics[width = 1\textwidth]{a13-pcaplot-1.eps}
\caption{Principal Components of training and attack subsets.}
\label{pca1}
\end{figure}
\begin{figure}
\includegraphics[width = 1\textwidth]{a13-pcaplot-2.eps}
\caption{Principal Components of training, attack and validation subsets.}
\label{pca2}
\end{figure}
% \subfloat{\includegraphics[width = 0.5\textwidth]{a13-pcaplot-2.eps}}  

Another way to improve our understanding of our dataset is to look at it's principal components. In order to compute the principal components we merge the whole dataset. As we can see in figures \ref{pca1} and \ref{pca2} we have used different colors for the three subsets of ADFA-LD 12. In the figure \ref{pca1} we only plot the training and attacks sets. In figure \ref{pca2} we also plot the validation set. While on the left figure there is some separation between the attack and the training set once we add the validation set we see that there is little separation between the attack dataset and the training and validation sets combined. The validation set's spatial distribution on the first two principal components is a bit different than the training set's. This means 
% research for training vs validation sets
that the training and the validation sets as provided do not carry the same information content. We can see how much of the variation of the dataset is captured with each principal component in 

% [ 0.20417851  0.12330985  0.09493127  0.0776562   0.06456913  0.0489988  0.03860293  0.03456735  0.03275491  0.03025882  0.02729846  0.02597533]


\chapter{Frequency Analysis of the ADFA-LD 12 Dataset}

We now proceed deeper in our analysis of the ADFA-LD 12 dataset. Our first goal is to replicate the results produced by Xie et al.\cite{adf1}. The paper focuses on frequency based feature engineering, similar to the one we used to explore our dataset. It's analysis is based in two algorithms, k-nearest neighbours and k-means clustering. One noteworthy part of \cite{adf1} is that the authors perform principal component analysis not on the whole of the dataset but only on the training set. We will keep their procedure while we are trying to replicate their results in the following sections.

\section{k-Nearest Neighbours}

We now describe the kNN implementation of Xie et al.\cite{adf1}, that we will replicate. Their work is done after they perform feature reduction through principal component analysis. They keep 80\% of the variance of the dataset, which results in them having a 9 dimensional feature space. The criterion for classifying a data point as normal or not is whether it has more than k data points from the training set within a distance d. The parameters k and d where chosen empirically from the authors and we will follow them. The authors used a variety of distance metrics for their exploration and we will do so as well.



\begin{figure}
\includegraphics[width = 0.9\textwidth]{b08-roc-1.eps}
\caption{Receiver Operating Characteristic curve for k-Nearest Neighbours with square euclidean distance.}
\label{pca1}
\end{figure}


\begin{figure}
\includegraphics[width = 0.9\textwidth]{b08-roc-2.eps}
\caption{Receiver Operating Characteristic curve for k-Nearest Neighbours with square standardised euclidean distance.}
\label{pca1}
\end{figure}


\begin{thebibliography}{99}


\bibitem{dat2} Gideon Creech, Jiankun Huy \emph{Generation of a new IDS Test Dataset: Time to Retire the KDD Collection}, 
2013 IEEE Wireless Communications and Networking Conference (WCNC)

\bibitem{intro-car1} Maurice Schellekens \emph{Car hacking: Navigating the regulatory landscape},\\ Computer Law \& Security Review 32 (2016) 307 - 315
%\\
%\url{http://www.sciencedirect.com/science/article/pii/S0267364915001843}

\bibitem{intro-cps1} Chih-Che Sun, Chen-Ching Liu and Jing Xie \emph{Cyber-Physical System Security of a Power Grid: State-of-the-Art}, MDPI - Electronics open access journal.
%\\
%\url{http://www.mdpi.com/2079-9292/5/3/40/pdf}

\bibitem{intro-ids1} Iti Raghav, Shashi Chhikara, Nitasha Hasteer \emph{Intrusion Detection and Prevention in Cloud Environment: A Systematic Review}, Journal of Network and Computer Applications, Volume 36, Issue 1, January 2013, Pages 25-41

\bibitem{out1} S. Roberts and L. Tarassenko, \emph{A probabilistic resource allocating
network for novelty detection}, Neural Computation, vol. 6, no. 2,
pp. 270 - 284, 1994.

\bibitem{out2} Lei A. Clifton, Hujun Yin, and Yang Zhang, \emph{Support Vector Machine in Novelty Detection for Multi-channel Combustion Data}, Proceedings of the third international conference on Advances in Neural Networks - Volume Part III (2006)

\bibitem{out3} P. Hayton, B. Scholkopf, L. Tarassenko, and P. Anuzis, \emph{Support vector novelty detection applied to jet engine vibration spectra}, in NIPS, pp. 946 - 952, 2000.

\bibitem{out4} 	Bernhard Schölkopf, John C. Platt, John C. Shawe-Taylor, Alex J. Smola, Robert C. Williamson, \emph{Estimating the Support of a High-Dimensional Distribution}, Neural Computation, Vol. 13, No. 7, pp. 1443-1471, 2001.


\bibitem{ids1} Monowar H. Bhuyan, D. K. Bhattacharyya, and J. K. Kalita (2014) \emph{Network Anomaly Detection: Methods, Systems and Tools}, IEEE Communications Surveys \& Tutorials, Vol. 16, No. 1, 2014

\bibitem{ids2} Mohiuddin Ahmed, Abdun Naser Mahmood, Jiankun Hu \emph{A survey of network anomaly detection techniques}, Journal of Network and Computer Applications. Vol. 60, January 2016, p. 19-31

\bibitem{adf1} Miao Xie, Jiankun Hu, Xinghuo Yu, and Elizabeth Chang \emph{Evaluating Host-Based Anomaly Detection Systems: Application of the Frequency-Based Algorithms to ADFA-LD}, 11th International Conference on Fuzzy Systems and Knowledge Discovery, 2014

\bibitem{adf2} Miao Xie, Jiankun Hu and Jill Slay \emph{Evaluating Host-based Anomaly Detection Systems:
Application of the One-class SVM Algorithm to ADFA-LD}, Proceedings of the 11th IEEE International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2014), Xiamen, 19-21 August 2014, 978-982. 

% python citation
\bibitem{pyt} G. van Rossum, \emph{Python tutorial, Technical Report CS-R9526}, Centrum voor Wiskunde en Informatica (CWI), Amsterdam, May 1995.

% numpy citation
\bibitem{numpy} David Ascher, Paul F. Dubois, Konrad Hinsen, James Hugunin, and Travis Oliphant,
\emph{Numerical Python} Lawrence Livermore National Laboratory, UCRL-MA-128569, 1999

% sk-learn citation
\bibitem{skl} F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot , E. Duchesnay 
\emph{Scikit-learn: Machine Learning in Python}, Journal of Machine Learning Research, volume 12,
 pages 2825 - 2830, 2011

%matplotlib
\bibitem{matpl} Hunter, J. D., \emph{Matplotlib: A 2D graphics environment}, Computing In Science \& Engineering, vol. 9, no. 3, pp 90 - 95, 2007



\end{thebibliography}


\end{document}
